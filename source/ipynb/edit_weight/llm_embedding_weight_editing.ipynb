{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6fced5e",
   "metadata": {},
   "source": [
    "\n",
    "# Practical LLM Embedding Weight Editing (with Tokenizer) + RAG Contrast\n",
    "\n",
    "이 노트북은 **Hugging Face 토크나이저/모델**을 이용해 **특정 토큰의 임베딩 가중치를 직접 수정**하고,\n",
    "수정 전후 출력/유사도를 비교합니다. 또한 **RAG(검색결합생성)** 방식과 비교하여,\n",
    "\"가중치를 바꾸는 것 vs. 바꾸지 않고 외부 지식을 넣는 것\"의 차이를 실감할 수 있게 합니다.\n",
    "\n",
    "> **필수/선택 의존성**\n",
    "> - 필수: `torch`, `transformers`\n",
    "> - 선택: `safetensors`, `scikit-learn`(RAG 섹션에서 간단한 최근접 이웃 검색용)\n",
    ">\n",
    "> 인터넷이 막힌 환경에서는 사전 다운로드한 모델 경로로 바꾸세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91831005",
   "metadata": {},
   "source": [
    "## 0) 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b358838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, platform\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "\n",
    "import importlib\n",
    "\n",
    "def has(mod):\n",
    "    try:\n",
    "        importlib.import_module(mod)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "HAS_TORCH = has(\"torch\")\n",
    "HAS_TRANSFORMERS = has(\"transformers\")\n",
    "HAS_SAFETENSORS = has(\"safetensors\")\n",
    "HAS_SKLEARN = has(\"sklearn\")\n",
    "\n",
    "print(\"torch:\", HAS_TORCH)\n",
    "print(\"transformers:\", HAS_TRANSFORMERS)\n",
    "print(\"safetensors:\", HAS_SAFETENSORS)\n",
    "print(\"sklearn:\", HAS_SKLEARN)\n",
    "\n",
    "if not (HAS_TORCH and HAS_TRANSFORMERS):\n",
    "    raise RuntimeError(\"This notebook requires torch and transformers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3b694",
   "metadata": {},
   "source": [
    "\n",
    "## 1) 토크나이저/모델 로드\n",
    "\n",
    "- 데모 모델: **`sshleifer/tiny-gpt2`** (아주 작은 GPT-2)  \n",
    "  - 인터넷이 없으면, 로컬 경로로 대체: `local_path = \"/path/to/local/tiny-gpt2\"`\n",
    "\n",
    "**주의:** tiny 모델은 품질이 낮지만 **빠른 실험**에 적합합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd280b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"sshleifer/tiny-gpt2\"  # or local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.eval();\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(\"Vocab size:\", model.config.vocab_size)\n",
    "print(\"Embedding shape:\", model.transformer.wte.weight.shape)  # [vocab, hidden]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39ec46",
   "metadata": {},
   "source": [
    "\n",
    "## 2) 기준 출력 생성 (수정 전)\n",
    "간단한 프롬프트로 출력 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93184d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(prompt, max_new_tokens=40, temperature=0.8, top_p=0.95):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "baseline = generate_text(prompt, max_new_tokens=30)\n",
    "print(\"=== BASELINE ===\")\n",
    "print(baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a65ba4",
   "metadata": {},
   "source": [
    "\n",
    "## 3) 특정 토큰 임베딩 관찰\n",
    "예시로 토큰 `\"France\"`의 토큰 ID를 확인하고, 해당 임베딩을 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3739d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tok = \"France\"\n",
    "tok_id = tokenizer.convert_tokens_to_ids(tok)\n",
    "if tok_id == tokenizer.unk_token_id:\n",
    "    # tiny-gpt2 토큰화가 다를 수 있으므로, 대안: 소문자/다른 서브워드 확인\n",
    "    tok = \"France\"\n",
    "    tok_id = tokenizer(tok)[\"input_ids\"][0]\n",
    "\n",
    "print(f\"Token '{tok}' -> id {tok_id}\")\n",
    "emb = model.transformer.wte.weight.data\n",
    "print(\"Original embedding (first 8 dims):\", emb[tok_id][:8].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d45f5c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) 임베딩 **직접 수정 (Weight Editing)**\n",
    "- `\"France\"` 임베딩을 인위적으로 이동시켜 봅니다.\n",
    "- 작은 델타를 더해 **미세한 편향**을 주입합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35314b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    delta = torch.zeros_like(emb[tok_id])\n",
    "    # 임의로 몇 차원에 작은 상수 주입 (실험 목적)\n",
    "    delta[:4] = torch.tensor([0.5, -0.4, 0.3, -0.2], device=emb.device)\n",
    "    emb[tok_id] = emb[tok_id] + 0.05 * delta\n",
    "\n",
    "print(\"Edited embedding (first 8 dims):\", emb[tok_id][:8].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56659a1e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) 수정 후 출력 비교\n",
    "같은 프롬프트로 다시 생성하여, 출력의 **변화**가 있는지 봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e71cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edited = generate_text(prompt, max_new_tokens=30)\n",
    "print(\"=== EDITED ===\")\n",
    "print(edited)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b197236",
   "metadata": {},
   "source": [
    "\n",
    "## 6) 임베딩 변화량 정량화 (코사인 유사도)\n",
    "수정 전/후 임베딩 벡터의 유사도를 계산합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 이전 값을 가지고 있지 않으니, 재현을 위해 역편집 샘플 생성\n",
    "    # (실제 실험에선 편집 전 벡터를 따로 저장해두는 것을 권장)\n",
    "    # 여기서는 편집 벡터를 빼서 \"원래 근처\"로 되돌린 복제본을 만들어 비교합니다.\n",
    "    # Note: 완벽히 같지는 않지만, 코사인 유사도 계산 예시로 충분합니다.\n",
    "    emb_now = emb[tok_id].detach().clone()\n",
    "    approx_prev = emb_now - 0.05 * torch.cat([torch.tensor([0.5, -0.4, 0.3, -0.2], device=emb.device), torch.zeros_like(emb_now[4:])])\n",
    "    cos_sim = F.cosine_similarity(emb_now.unsqueeze(0), approx_prev.unsqueeze(0)).item()\n",
    "\n",
    "print(\"Approx cosine similarity (edited vs approx-prev):\", cos_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0cb272",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (선택) safetensors로 저장/로드\n",
    "편집된 임베딩을 포함한 `state_dict`를 safetensors로 저장/로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ba7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if HAS_SAFETENSORS:\n",
    "    from safetensors.torch import save_file, load_file\n",
    "    state = model.state_dict()\n",
    "    save_file(state, \"/mnt/data/tinygpt2_edited.safetensors\")\n",
    "    print(\"Saved: /mnt/data/tinygpt2_edited.safetensors\")\n",
    "\n",
    "    loaded = load_file(\"/mnt/data/tinygpt2_edited.safetensors\")\n",
    "    # 새 모델에 주입 (동일 아키텍처 필요)\n",
    "    model2 = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    model2.load_state_dict(loaded, strict=True)\n",
    "    print(\"Reloaded edited weights into a fresh model instance.\")\n",
    "else:\n",
    "    print(\"safetensors not available — skipping save/load demo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bddab6",
   "metadata": {},
   "source": [
    "\n",
    "## 8) RAG(검색결합생성) 간단 대비 실험\n",
    "\n",
    "가중치를 바꾸지 않고, **외부 지식**을 검색해서 프롬프트에 넣는 방식을 시뮬레이션합니다.\n",
    "여기서는 작은 문서 코퍼스를 만들어, 쿼리와 **TF-IDF 유사도**(또는 최근접 이웃)로 상위 문서를 찾아\n",
    "프롬프트에 붙여 넣습니다. (간단한 데모)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0abb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = [\n",
    "    \"Paris is the capital of France. It is known for the Eiffel Tower.\",\n",
    "    \"Berlin is the capital of Germany. It has the Brandenburg Gate.\",\n",
    "    \"Rome is the capital of Italy. It has the Colosseum.\"\n",
    "]\n",
    "\n",
    "query = \"What is the capital of France?\"\n",
    "\n",
    "if HAS_SKLEARN:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "\n",
    "    vec = TfidfVectorizer().fit(docs + [query])\n",
    "    X = vec.transform(docs)\n",
    "    q = vec.transform([query])\n",
    "    sims = cosine_similarity(q, X)[0]\n",
    "    top_idx = int(np.argmax(sims))\n",
    "    retrieved = docs[top_idx]\n",
    "else:\n",
    "    # fallback: 가장 단순한 키워드 포함 수 카운트\n",
    "    def score(doc, q):\n",
    "        s = 0\n",
    "        for w in q.lower().split():\n",
    "            if w in doc.lower():\n",
    "                s += 1\n",
    "        return s\n",
    "    scored = [(i, score(d, query)) for i, d in enumerate(docs)]\n",
    "    scored.sort(key=lambda t: t[1], reverse=True)\n",
    "    top_idx = scored[0][0]\n",
    "    retrieved = docs[top_idx]\n",
    "\n",
    "rag_prompt = f\"Context: {retrieved}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "rag_out = generate_text(rag_prompt, max_new_tokens=40)\n",
    "print(\"=== RAG RESULT ===\")\n",
    "print(rag_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017c228",
   "metadata": {},
   "source": [
    "\n",
    "## 9) 체크리스트 & 요약\n",
    "- 토큰 임베딩 **직접 편집**은 빠르지만 **예측 불가**한 부작용이 있을 수 있습니다.\n",
    "- **safetensors**로 안전하게 배포/공유하세요.\n",
    "- **RAG**는 가중치를 건드리지 않고 **최신성/사내 데이터**를 반영할 수 있는 실무 친화적 방법입니다.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
