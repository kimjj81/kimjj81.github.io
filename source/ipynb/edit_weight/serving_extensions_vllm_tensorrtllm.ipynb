{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36889344",
   "metadata": {},
   "source": [
    "\n",
    "# Serving Extensions — vLLM & TensorRT-LLM (Cookbook)\n",
    "\n",
    "이 노트북은 **vLLM**과 **TensorRT-LLM**을 활용한 서빙 확장 전반을 다룹니다.\n",
    "- vLLM: PagedAttention, OpenAI-호환 서버, 동시성/처리량 설정\n",
    "- TensorRT-LLM: 모델 변환 → 엔진 빌드 → 런타임 추론 (NVIDIA GPU 전용)\n",
    "\n",
    "> 이 노트북은 환경에 따라 일부 셀은 **가이드/샘플 코드** 형태입니다.\n",
    "> 실제 실행에는 해당 라이브러리/도구 설치와 GPU가 필요합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f94d0e",
   "metadata": {},
   "source": [
    "## 1) vLLM — Quickstart & API Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (가이드) 설치\n",
    "# pip install vllm  # CUDA 환경 필요\n",
    "\n",
    "print(\"vLLM quickstart — this cell contains guidance. Install vLLM in your environment to run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (샘플) 파이썬 내장 추론\n",
    "# from vllm import LLM, SamplingParams\n",
    "# llm = LLM(model=\"meta-llama/Llama-3-8B\", tensor_parallel_size=1)  # 모델/권한 필요\n",
    "# sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=64)\n",
    "# outputs = llm.generate([\"Hello, LLM!\"], sampling_params)\n",
    "# print(outputs[0].outputs[0].text)\n",
    "\n",
    "print(\"See commented code for Python in-process vLLM usage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5f63d",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 OpenAI 호환 서버 실행 (권장)\n",
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model meta-llama/Llama-3-8B \\\n",
    "  --tensor-parallel-size 2 \\\n",
    "  --gpu-memory-utilization 0.90 \\\n",
    "  --max-num-batched-token 8192 \\\n",
    "  --port 8000\n",
    "```\n",
    "- **중요 옵션**\n",
    "  - `--tensor-parallel-size`: 멀티 GPU 분산\n",
    "  - `--gpu-memory-utilization`: VRAM 사용 상한\n",
    "  - `--max-num-batched-token`: 동시성/처리량에 큰 영향\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112bba57",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 OpenAI 클라이언트로 호출 (Python)\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3-8B\",\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Explain PagedAttention briefly.\"}],\n",
    "    temperature=0.7, top_p=0.9\n",
    ")\n",
    "print(resp.choices[0].message.content)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d49933",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 성능 팁\n",
    "- **`max_num_batched_token`**와 **`block_size`** 튜닝 → 처리량에 직접적 영향\n",
    "- **KV 캐시 Offload**(CPU/디스크) 고려: 장문/동시성↑시 유용\n",
    "- **CUDA 그래프**/Pinned memory 설정으로 지연시간 하향\n",
    "- **prompt caching** 활성화로 반복 프롬프트 비용 절감\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f020abd",
   "metadata": {},
   "source": [
    "## 2) TensorRT-LLM — Build & Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8845ed",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 변환 및 엔진 빌드 개요\n",
    "1. **HF 모델 → TensorRT-LLM 포맷 변환**\n",
    "2. **엔진 빌드 (INT8/FP8/FP16 등 정밀도 선택)**\n",
    "3. **런타임 추론 (`trtllm-run` 등)**\n",
    "\n",
    "> 자세한 명령은 TensorRT-LLM 버전에 따라 다를 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (가이드) 설치 및 버전 확인\n",
    "# pip install tensorrt_llm\n",
    "print(\"TensorRT-LLM steps are provided as guidance. Requires NVIDIA GPU/driver and TRT-LLM installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ac41f",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 변환 예시 (명령줄)\n",
    "```bash\n",
    "# 예시: Llama-3-8B를 TRT-LLM용으로 변환 (경로/옵션은 환경에 맞게 변경)\n",
    "trtllm-build \\\n",
    "  --model_dir /models/Llama-3-8B \\\n",
    "  --dtype float16 \\\n",
    "  --tp_size 2 \\\n",
    "  --pp_size 1 \\\n",
    "  --output_dir /models/Llama-3-8B-trt\n",
    "```\n",
    "- `--dtype`: `float16`, `fp8`, `int8` 등 선택\n",
    "- `--tp_size`/`--pp_size`: 텐서/파이프라인 병렬화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96f083",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 런타임 추론\n",
    "```bash\n",
    "trtllm-run \\\n",
    "  --engine_dir /models/Llama-3-8B-trt \\\n",
    "  --max_output_len 128 \\\n",
    "  --tokenizer_dir /models/Llama-3-8B\n",
    "```\n",
    "- 서버 모드(HTTP/gRPC) 실행 스크립트는 배포판/샘플에 포함되어 있는 경우가 많습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2268c6",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 성능 팁\n",
    "- **정밀도**: `fp8`/`int8`로 내리면 처리량↑, 품질/안정성은 테스트 필요\n",
    "- **프로파일링**: `nsys`, `ncu`로 커널/메모리 병목 확인\n",
    "- **배치/패킹**: 긴 프롬프트 혼합 시 패킹으로 토큰 낭비 최소화\n",
    "- **엔진 재사용**: 동일 모델/설정에서는 엔진 캐시 활용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef8e61",
   "metadata": {},
   "source": [
    "## 3) 미니 벤치마크 스캐폴드 (공통)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94829eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 이 스캐폴드는 어떤 서빙 백엔드든 HTTP OpenAI 호환 엔드포인트가 있으면\n",
    "# 간단히 처리량/지연시간을 측정할 수 있습니다.\n",
    "\n",
    "import time, statistics, json, os\n",
    "import threading\n",
    "import queue\n",
    "import requests\n",
    "\n",
    "def load_prompts(n=50):\n",
    "    base = \"Write a short haiku about {}.\"\n",
    "    topics = [f\"topic-{i}\" for i in range(n)]\n",
    "    return [base.format(t) for t in topics]\n",
    "\n",
    "def worker(url, model, prompts, results):\n",
    "    for p in prompts:\n",
    "        t0 = time.time()\n",
    "        r = requests.post(\n",
    "            f\"{url}/chat/completions\",\n",
    "            headers={\"Authorization\": \"Bearer EMPTY\", \"Content-Type\":\"application/json\"},\n",
    "            data=json.dumps({\n",
    "                \"model\": model,\n",
    "                \"messages\":[{\"role\":\"user\",\"content\":p}],\n",
    "                \"temperature\":0.7,\n",
    "                \"top_p\":0.9,\n",
    "                \"max_tokens\":64,\n",
    "                \"stream\": False\n",
    "            }),\n",
    "            timeout=60\n",
    "        )\n",
    "        dt = time.time() - t0\n",
    "        results.put(dt)\n",
    "\n",
    "def run_benchmark(url=\"http://localhost:8000/v1\", model=\"your-model\", concurrency=8, total=64):\n",
    "    prompts = load_prompts(total)\n",
    "    per_worker = total // concurrency\n",
    "    results = queue.Queue()\n",
    "\n",
    "    threads = []\n",
    "    for i in range(concurrency):\n",
    "        subset = prompts[i*per_worker:(i+1)*per_worker]\n",
    "        th = threading.Thread(target=worker, args=(url, model, subset, results))\n",
    "        th.start()\n",
    "        threads.append(th)\n",
    "\n",
    "    for th in threads: th.join()\n",
    "\n",
    "    times = []\n",
    "    while not results.empty():\n",
    "        times.append(results.get())\n",
    "\n",
    "    if times:\n",
    "        p50 = statistics.median(times)\n",
    "        p90 = sorted(times)[int(0.9*len(times))-1]\n",
    "        thpt = len(times) / sum(times)\n",
    "        print(f\"Requests: {len(times)}  |  p50: {p50:.3f}s  p90: {p90:.3f}s  |  Throughput: {thpt:.2f} req/s\")\n",
    "    else:\n",
    "        print(\"No results collected. Check server URL/model and retry.\")\n",
    "\n",
    "print(\"Benchmark scaffold ready. Start your vLLM/TensorRT-LLM server and call run_benchmark().\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67e2b8",
   "metadata": {},
   "source": [
    "\n",
    "## 4) 마무리 팁\n",
    "- **vLLM**: 동시성/배치 토큰 설정이 성능 핵심. KV 캐시 블록 사이즈와 메모리 사용률을 조절.\n",
    "- **TensorRT-LLM**: 정밀도/병렬화 설정이 핵심. 변환/엔진 빌드는 느릴 수 있으나, 한 번 빌드하면 매우 빠름.\n",
    "- 공통: **프롬프트 캐시**와 **장문 프롬프트 패킹**으로 토큰 낭비를 줄이세요.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
