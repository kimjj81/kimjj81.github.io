---
title: LLM 서빙 관련 정리
date: 2025-08-12 15:58:42
slug: llm-serving-summary
tags: llm-serving, vllm, tensorrtllm
---

# 📌 LLM 서빙 관련 핵심 정리

## 개요
- **LLM 서빙 도구**(MLC, CTranslate2, vLLM, TensorRT-LLM 등)는 **오픈소스 LLM 가중치**를 GPU/CPU/모바일 등 다양한 환경에서 **고성능·저지연**으로 제공하기 위한 프레임워크.
- **폐쇄형 모델**(OpenAI GPT, Claude, Gemini 등)은 **가중치 비공개** → 직접 서빙 불가능, **API 기반**으로만 사용.
- **Azure OpenAI / AWS Bedrock**: 모델 가중치를 제공하지 않는 **관리형 LLM 서비스**. 직접 수정은 불가, 제한된 Fine-tuning API만 제공.
- **자체 서빙 이유**: 보안, 데이터 소유권, 비용 절감, 커스터마이징, 성능 제어.
- **모델 수정 방식**:
  - **가중치 수정**: 모델을 뜯어서 파라미터를 직접 고치는 것. 직접 weight를 변경 → 관리형 LLM 불가. 
  - **Fine-tuning**: 제한된 API로 특정 태스크/도메인 학습.
  - **RAG**: 외부 지식을 검색해 모델 입력에 주입.

---

## 1. LLM 서빙 도구 성능/메모리/동시성/하드웨어 지원 비교

| 도구 | 성능(추론 속도) | 메모리 효율 | 동시성 처리 | 하드웨어 지원 |
|------|----------------|-------------|-------------|---------------|
| **MLC-LLM** | 중~상 (하드웨어별 최적화 필요) | 우수 (양자화, 오프로드 지원) | 중간 수준 | CPU, GPU, NPU, WebGPU, 모바일 |
| **CTranslate2** | 중간 (CPU 최적화 강점) | 매우 우수 (저메모리 운영) | 제한적 | CPU, GPU (NVIDIA, Intel) |
| **vLLM** | 매우 높음 (PagedAttention) | 우수 (효율적 KV 캐시) | 매우 높음 (대규모 동시 요청 처리) | GPU 중심 (CUDA) |
| **TensorRT-LLM** | 최고 (NVIDIA GPU 전용 최적화) | 우수 (FP8, INT4) | 높음 | NVIDIA GPU 전용 |
| **TGI** | 높음 (FlashAttention) | 우수 | 높음 | GPU 중심 |
| **llama.cpp** | 낮음~중간 (CPU 기반) | 매우 우수 (int4/int8 양자화) | 낮음 | CPU, 모바일, WebAssembly |

---

## 2. API 기반 vs 자체 서빙 기반 비교

  
| 항목 | API 기반 (OpenAI, Claude API 등) | 자체 서빙 (vLLM, TensorRT-LLM 등) |
|------|---------------------------------|-----------------------------------|
| **보안** | 외부 전송 필요 → 기밀 유출 위험 | 사내망/온프레미스 → 높은 보안성 |
| **비용** | 소규모 저렴, 대규모 고비용 | 초기 비용↑, 대규모 장기 저렴 |
| **운영 난이도** | 쉬움 (서비스 제공사 관리) | 높음 (모델 배포·모니터링 필요) |
| **유연성** | 모델 구조/가중치 수정 불가 | 자유로운 수정·최적화 가능 |
| **성능 제어** | 제공사 SLA 의존 | 하드웨어·소프트웨어 직접 튜닝 가능 |
| **업데이트** | 자동 최신 모델 제공 | 직접 업그레이드 필요 |

---

## 3. 가중치 수정 vs Fine-tuning vs RAG 비교

* 관리형 LLM에서의 Fine-tuning 제한 사항
  - 원본 가중치 접근 불가
  - 커스텀 학습 데이터 업로드만 가능
  - 학습된 결과물은 API 형태로만 호출 가능

| 구분 | 가중치 수정 (Weight Editing) | Fine-tuning | RAG (검색결합생성) |
|------|-----------------------------|-------------|--------------------|
| **정의** | 특정 레이어 파라미터 직접 변경 | 새 데이터로 모델 재학습 | 외부 지식 검색 후 입력에 결합 |
| **목적** | 특정 지식/편향 수정, 미세 변경 | 도메인/태스크 특화 | 최신 정보 반영, 지식 확장 |
| **수정 범위** | 단일 파라미터~전체 가능 | 전체/부분 레이어 | 모델 가중치 변경 없음 |
| **데이터 필요** | 없음 또는 소량 | 학습 데이터 필수 | 문서·DB 등 외부 자료 |
| **속도/비용** | 빠름(소규모 변경) | 느림(학습 필요) | 빠름(검색+추론) |
| **관리형 LLM에서 가능 여부** | ❌ 불가 | ✅ 일부 가능 (API 제공 범위 내) | ✅ 가능 |
| **예시** | 모델에 저장된 잘못된 사실 수정 | GPT-3.5를 법률 문서 작성 특화 | 내부 문서 검색 후 답변 |

---

## 서빙 성능 최적화 실습

다운로드 링크 [serving_extensions_vllm_tensorrtllm.ipynb](https://github.com/kimjj81/kimjj81.github.io/blob/master/source/ipynb/edit_weight/serving_extensions_vllm_tensorrtllm.ipynb)

**vLLM**과 **TensorRT-LLM**을 이용한 고성능 LLM 서빙 환경 구축 가이드와 성능 튜닝 실습.

**주요 내용:**
- vLLM Python API 및 OpenAI 호환 서버 실행 방법
- 동시성/배치/메모리 최적화 옵션 (`max_num_batched_token`, `gpu_memory_utilization` 등)
- TensorRT-LLM 변환/엔진 빌드 명령어 예시 (`fp16`, `fp8`, `int8` 정밀도 선택)
- 런타임 추론, 서버 모드 구성, 병렬화 전략
- OpenAI 호환 엔드포인트 성능 측정 미니 벤치마크 코드
- 실전 성능 최적화 체크리스트

------
GPT-5, Claude-4 를 통해 생성된 문서를 수정